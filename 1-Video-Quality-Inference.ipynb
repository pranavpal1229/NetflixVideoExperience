{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "zbGBJegvBuXV"
   },
   "source": [
    "### Extract Features from the Network Traffic\n",
    "\n",
    "Load the `netflix.pcap` file, which is a packet trace that includes network traffic. \n",
    "\n",
    "Click [here](https://github.com/noise-lab/ml-systems/blob/main/docs/notebooks/data/netflix.pcap) to download `netflix.pcap`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scapy.all import rdpcap, IP, TCP, UDP, DNS\n",
    "import pandas as pd\n",
    "\n",
    "# Read pcap and build rows with a robust 'length' computation\n",
    "pkts = rdpcap(\"../../myData/netflix.pcap\")\n",
    "rows = []\n",
    "for pkt in pkts:\n",
    "    is_DNS = DNS in pkt\n",
    "    proto = \"DNS\" if is_DNS else (\"TCP\" if TCP in pkt else (\"UDP\" if UDP in pkt else \"Other\"))\n",
    "    # compute packet length robustly; len(pkt) normally works for scapy packets\n",
    "    try:\n",
    "        pkt_len = len(pkt)\n",
    "    except Exception:\n",
    "        pkt_len = getattr(pkt, 'len', None)\n",
    "    rows.append({\n",
    "        \"timestamp\": float(getattr(pkt, \"time\", 0)),\n",
    "        \"length\": pkt_len,\n",
    "        \"src_ip\": pkt[IP].src if IP in pkt else None,\n",
    "        \"dst_ip\": pkt[IP].dst if IP in pkt else None,\n",
    "        \"txn_id\": pkt[DNS].id if is_DNS else None,\n",
    "        \"protocol\": proto,\n",
    "        \"src_port\": pkt.sport if (TCP in pkt or UDP in pkt) else None,\n",
    "        \"dst_port\": pkt.dport if (TCP in pkt or UDP in pkt) else None,\n",
    "        \"info\": str(pkt.summary())\n",
    "    })\n",
    "pcap__import = pd.DataFrame(rows)\n",
    "\n",
    "# Normalise: ensure there is a numeric 'length' column for downstream processing\n",
    "if 'length' not in pcap__import.columns and 'len' in pcap__import.columns:\n",
    "    pcap__import['length'] = pcap__import['len']\n",
    "# coerce to numeric and use pandas nullable integer dtype where possible\n",
    "if 'length' in pcap__import.columns:\n",
    "    pcap__import['length'] = pd.to_numeric(pcap__import['length'], errors='coerce').astype('Int64')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "netflix_dns = pcap__import[\n",
    "    pcap__import[\"info\"].str.contains(r\"netflix|nflx\", case=False, na=False)\n",
    "]\n",
    "print(netflix_dns.head(15))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "TOPPsKpYB6PS"
   },
   "source": [
    "### Identifying the Service Type\n",
    "\n",
    "Use the DNS traffic to filter the packet trace for Netflix traffic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# only netflix queries (not answers)\n",
    "netflix_qry = pcap__import[\n",
    "    pcap__import[\"info\"].str.contains(r\"(netflix|nflx)\", case=False, na=False)\n",
    "    & pcap__import[\"info\"].str.contains(\"Qry\", case=False, na=False)\n",
    "]\n",
    "\n",
    "# show the txn ids and a few context columns\n",
    "print(netflix_qry[[\"timestamp\",\"src_ip\",\"dst_ip\",\"txn_id\",\"info\"]].head(30))\n",
    "#drop duplicates\n",
    "netflix_qry = netflix_qry.drop_duplicates(subset=[\"txn_id\"])\n",
    "print(netflix_qry[[\"timestamp\",\"src_ip\",\"dst_ip\",\"txn_id\",\"info\"]].head(30))\n",
    "#create a set of txn ids\n",
    "netflix_txn_ids = set(netflix_qry[\"txn_id\"])\n",
    "print(netflix_txn_ids)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#now we want to loop through netflix_dns and find all the answers that match the txn ids\n",
    "nflx_answers = pcap__import[pcap__import[\"txn_id\"].isin(netflix_txn_ids) & pcap__import[\"info\"].str.contains(\"Ans\", case=False, na=False) & pcap__import[\"protocol\"].eq(\"DNS\")] \n",
    "nflx_answers.head()\n",
    "print(len(nflx_answers))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#now let us extract the ips of nflx answers\n",
    "nflx_ips = set()\n",
    "for info in nflx_answers[\"info\"]:\n",
    "    parts = info.split(\"\\\"\")\n",
    "    for part in parts:\n",
    "        if part.count(\".\") == 3:  # crude check for an IP address\n",
    "            nflx_ips.add(part)  \n",
    "print(nflx_ips)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "0qFV6q2OCCsK"
   },
   "source": [
    "### Generate Statistics\n",
    "\n",
    "Generate statistics and features for the Netflix traffic flows. Use the `netml` library or any other technique that you choose to generate a set of features that you think would be good features for your model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nflx_pkts = pcap__import[\n",
    "    (pcap__import[\"src_ip\"].isin(nflx_ips)) | (pcap__import[\"dst_ip\"].isin(nflx_ips))\n",
    "]\n",
    "\n",
    "nflx_pkts.head()\n",
    "\n",
    "#I want to get some statistics that I can use about the nflx packets and traffic flows\n",
    "print(len(nflx_pkts))\n",
    "print(nflx_pkts[\"length\"].describe())\n",
    "\n",
    "#now lets get some more statistics to help us truly understand the traffic flows and provide features for our MOdel\n",
    "nflx_pkts.describe()\n",
    "\n",
    "#I also want to see how to the packets are distributed over time\n",
    "nflx_pkts[\"timestamp\"].plot(kind='hist', bins=50, title='Distribution of Packet Timestamps', xlabel='Timestamp', ylabel='Frequency')\n",
    "#other features that might be useful are inter-arrival times, packet sizes, burstiness, protocol distribution, flow durations, and packet counts per flow.\n",
    "#inter arrival time: \n",
    "print(\"inter-arrival time statistics:\")\n",
    "nflx_pkts = nflx_pkts.sort_values(by=\"timestamp\")\n",
    "nflx_pkts[\"inter_arrival_time\"] = nflx_pkts[\"timestamp\"].diff().fillna(0)\n",
    "print(nflx_pkts[\"inter_arrival_time\"].describe())\n",
    "\n",
    "#burstiness: standard deviation of inter-arrival times\n",
    "burstiness = nflx_pkts[\"inter_arrival_time\"].std()\n",
    "print(f\"Burstiness (std of inter-arrival times): {burstiness}\")\n",
    "\n",
    "#protocol distribution which daescribes \n",
    "protocol_counts = nflx_pkts[\"protocol\"].value_counts(normalize=True)\n",
    "print(\"Protocol Distribution:\")\n",
    "print(protocol_counts)\n",
    "\n",
    "#flow durations and packet counts per flow\n",
    "nflx_pkts[\"flow_id\"] = nflx_pkts.apply(lambda row: f\"{row['src_ip']}-{row['dst_ip']}-{row['src_port']}-{row['dst_port']}-{row['protocol']}\", axis=1)\n",
    "flow_stats = nflx_pkts.groupby(\"flow_id\").agg(\n",
    "    flow_duration=pd.NamedAgg(column=\"timestamp\", aggfunc=lambda x: x.max() - x.min()),\n",
    "    packet_count=pd.NamedAgg(column=\"timestamp\", aggfunc=\"count\")\n",
    ").reset_index()\n",
    "print(\"Flow Statistics:\")\n",
    "print(flow_stats.describe())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "0qFV6q2OCCsK"
   },
   "source": [
    "**Write a brief justification for the features that you have chosen.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "lBCP_2SBC2xj"
   },
   "source": [
    "### Inferring Segment downloads\n",
    "\n",
    "In addition to the features that you could generate using the `netml` library or similar, add to your feature vector a \"segment downloads rate\" feature, which indicates the number of video segments downloaded for a given time window.\n",
    "\n",
    "Note: If you are using the `netml` library, generating features with `SAMP` style options may be useful, as this option gives you time windows, and you can then simply add the segment download rate to that existing dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#add to your feature vector a \"segment downloads rate\" feature, which indicates the number of video segments downloaded for a given time window.\n",
    "nflx_pkts[\"time_window\"] = (nflx_pkts[\"timestamp\"] // 10) * 10  # 10-second windows\n",
    "segment_download_rate = nflx_pkts.groupby(\"time_window\").size().reset_index(name=\"segment_download_rate\")\n",
    "nflx_pkts = nflx_pkts.merge(segment_download_rate, on=\"time_window\", how=\"left\")\n",
    "print(nflx_pkts[[\"timestamp\", \"time_window\", \"segment_download_rate\"]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Video Quality Inference\n",
    "\n",
    "You will now load the complete video dataset from a previous study to train and test models based on these features to automatically infer the quality of a streaming video flow.\n",
    "\n",
    "For this part of the assignment, you will need two pickle files, which we provide for you by running the code below:\n",
    "\n",
    "```\n",
    "\n",
    "!gdown 'https://drive.google.com/uc?id=1N-Cf4dJ3fpak_AWgO05Fopq_XPYLVqdS' -O netflix_session.pkl\n",
    "!gdown 'https://drive.google.com/uc?id=1PHvEID7My6VZXZveCpQYy3lMo9RvMNTI' -O video_dataset.pkl\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the File\n",
    "\n",
    "Load the video dataset pickle file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!gdown 'https://drive.google.com/uc?id=1N-Cf4dJ3fpak_AWgO05Fopq_XPYLVqdS' -O netflix_session.pkl\n",
    "!gdown 'https://drive.google.com/uc?id=1PHvEID7My6VZXZveCpQYy3lMo9RvMNTI' -O video_dataset.pkl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clean the File\n",
    "\n",
    "1. The dataset contains video resolutions that are not valid. Remove entries in the dataset that do not contain a valid video resolution. Valid resolutions are 240, 360, 480, 720, 1080."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import pandas as pd\n",
    "\n",
    "with open('../../myData/netflix_dataset.pkl', 'rb') as f:\n",
    "    session_data = pickle.load(f)\n",
    "\n",
    "print(session_data.columns.tolist())\n",
    "print(session_data.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. The file also contains columns that are unnecessary (in fact, unhelpful!) for performing predictions. Identify those columns, and remove them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Briefly explain why you removed those columns.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare Your Data\n",
    "\n",
    "Prepare your data matrix, determine your features and labels, and perform a train-test split on your data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train and Tune Your Model\n",
    "\n",
    "1. Select a model of your choice.\n",
    "2. Train the model using your training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tune Your Model\n",
    "\n",
    "Perform hyperparameter tuning to find optimal parameters for your model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate Your Model\n",
    "\n",
    "Evaluate your model accuracy according to the following metrics:\n",
    "\n",
    "1. Accuracy\n",
    "2. F1 Score\n",
    "3. Confusion Matrix\n",
    "4. ROC/AUC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Predict the Ongoing Resolution of a Real Netflix Session\n",
    "\n",
    "Now that you have your model, it's time to put it in practice!\n",
    "\n",
    "Use a preprocessed Netflix video session to infer **and plot** the resolution at 10-second time intervals."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyM0Cd1i3qtplCsVAB9qTjxA",
   "collapsed_sections": [],
   "name": "pcap_processing_solution.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
