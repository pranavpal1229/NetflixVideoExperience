{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "zbGBJegvBuXV"
   },
   "source": [
    "### Extract Features from the Network Traffic\n",
    "\n",
    "Load the `netflix.pcap` file, which is a packet trace that includes network traffic. \n",
    "\n",
    "Click [here](https://github.com/noise-lab/ml-systems/blob/main/docs/notebooks/data/netflix.pcap) to download `netflix.pcap`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scapy.all import rdpcap, IP, TCP, UDP, DNS\n",
    "import pandas as pd\n",
    "\n",
    "# Read pcap and build rows with a robust 'length' computation\n",
    "pkts = rdpcap(\"../../myData/netflix.pcap\")\n",
    "rows = []\n",
    "for pkt in pkts:\n",
    "    is_DNS = DNS in pkt\n",
    "    proto = \"DNS\" if is_DNS else (\"TCP\" if TCP in pkt else (\"UDP\" if UDP in pkt else \"Other\"))\n",
    "    # compute packet length robustly; len(pkt) normally works for scapy packets\n",
    "    try:\n",
    "        pkt_len = len(pkt)\n",
    "    except Exception:\n",
    "        pkt_len = getattr(pkt, 'len', None)\n",
    "    rows.append({\n",
    "        \"timestamp\": float(getattr(pkt, \"time\", 0)),\n",
    "        \"length\": pkt_len,\n",
    "        \"src_ip\": pkt[IP].src if IP in pkt else None,\n",
    "        \"dst_ip\": pkt[IP].dst if IP in pkt else None,\n",
    "        \"txn_id\": pkt[DNS].id if is_DNS else None,\n",
    "        \"protocol\": proto,\n",
    "        \"src_port\": pkt.sport if (TCP in pkt or UDP in pkt) else None,\n",
    "        \"dst_port\": pkt.dport if (TCP in pkt or UDP in pkt) else None,\n",
    "        \"info\": str(pkt.summary())\n",
    "    })\n",
    "pcap__import = pd.DataFrame(rows)\n",
    "\n",
    "# Normalise: ensure there is a numeric 'length' column for downstream processing\n",
    "if 'length' not in pcap__import.columns and 'len' in pcap__import.columns:\n",
    "    pcap__import['length'] = pcap__import['len']\n",
    "# coerce to numeric and use pandas nullable integer dtype where possible\n",
    "if 'length' in pcap__import.columns:\n",
    "    pcap__import['length'] = pd.to_numeric(pcap__import['length'], errors='coerce').astype('Int64')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "netflix_dns = pcap__import[\n",
    "    pcap__import[\"info\"].str.contains(r\"netflix|nflx\", case=False, na=False)\n",
    "]\n",
    "print(netflix_dns.head(15))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "TOPPsKpYB6PS"
   },
   "source": [
    "### Identifying the Service Type\n",
    "\n",
    "Use the DNS traffic to filter the packet trace for Netflix traffic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# only netflix queries (not answers)\n",
    "netflix_qry = pcap__import[\n",
    "    pcap__import[\"info\"].str.contains(r\"(netflix|nflx)\", case=False, na=False)\n",
    "    & pcap__import[\"info\"].str.contains(\"Qry\", case=False, na=False)\n",
    "]\n",
    "\n",
    "# show the txn ids and a few context columns\n",
    "print(netflix_qry[[\"timestamp\",\"src_ip\",\"dst_ip\",\"txn_id\",\"info\"]].head(30))\n",
    "#drop duplicates\n",
    "netflix_qry = netflix_qry.drop_duplicates(subset=[\"txn_id\"])\n",
    "print(netflix_qry[[\"timestamp\",\"src_ip\",\"dst_ip\",\"txn_id\",\"info\"]].head(30))\n",
    "#create a set of txn ids\n",
    "netflix_txn_ids = set(netflix_qry[\"txn_id\"])\n",
    "print(netflix_txn_ids)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#now we want to loop through netflix_dns and find all the answers that match the txn ids\n",
    "nflx_answers = pcap__import[pcap__import[\"txn_id\"].isin(netflix_txn_ids) & pcap__import[\"info\"].str.contains(\"Ans\", case=False, na=False) & pcap__import[\"protocol\"].eq(\"DNS\")] \n",
    "nflx_answers.head()\n",
    "print(len(nflx_answers))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#now let us extract the ips of nflx answers\n",
    "nflx_ips = set()\n",
    "for info in nflx_answers[\"info\"]:\n",
    "    parts = info.split(\"\\\"\")\n",
    "    for part in parts:\n",
    "        if part.count(\".\") == 3:  # crude check for an IP address\n",
    "            nflx_ips.add(part)  \n",
    "print(nflx_ips)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "0qFV6q2OCCsK"
   },
   "source": [
    "### Generate Statistics\n",
    "\n",
    "Generate statistics and features for the Netflix traffic flows. Use the `netml` library or any other technique that you choose to generate a set of features that you think would be good features for your model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nflx_pkts = pcap__import[\n",
    "    (pcap__import[\"src_ip\"].isin(nflx_ips)) | (pcap__import[\"dst_ip\"].isin(nflx_ips))\n",
    "]\n",
    "\n",
    "nflx_pkts.head()\n",
    "\n",
    "#I want to get some statistics that I can use about the nflx packets and traffic flows\n",
    "print(len(nflx_pkts))\n",
    "print(nflx_pkts[\"length\"].describe())\n",
    "\n",
    "#now lets get some more statistics to help us truly understand the traffic flows and provide features for our MOdel\n",
    "nflx_pkts.describe()\n",
    "\n",
    "#I also want to see how to the packets are distributed over time\n",
    "nflx_pkts[\"timestamp\"].plot(kind='hist', bins=50, title='Distribution of Packet Timestamps', xlabel='Timestamp', ylabel='Frequency')\n",
    "#other features that might be useful are inter-arrival times, packet sizes, burstiness, protocol distribution, flow durations, and packet counts per flow.\n",
    "#inter arrival time: \n",
    "print(\"inter-arrival time statistics:\")\n",
    "nflx_pkts = nflx_pkts.sort_values(by=\"timestamp\")\n",
    "nflx_pkts[\"inter_arrival_time\"] = nflx_pkts[\"timestamp\"].diff().fillna(0)\n",
    "print(nflx_pkts[\"inter_arrival_time\"].describe())\n",
    "\n",
    "#burstiness: standard deviation of inter-arrival times\n",
    "burstiness = nflx_pkts[\"inter_arrival_time\"].std()\n",
    "print(f\"Burstiness (std of inter-arrival times): {burstiness}\")\n",
    "\n",
    "#protocol distribution which daescribes \n",
    "protocol_counts = nflx_pkts[\"protocol\"].value_counts(normalize=True)\n",
    "print(\"Protocol Distribution:\")\n",
    "print(protocol_counts)\n",
    "\n",
    "#flow durations and packet counts per flow\n",
    "nflx_pkts[\"flow_id\"] = nflx_pkts.apply(lambda row: f\"{row['src_ip']}-{row['dst_ip']}-{row['src_port']}-{row['dst_port']}-{row['protocol']}\", axis=1)\n",
    "flow_stats = nflx_pkts.groupby(\"flow_id\").agg(\n",
    "    flow_duration=pd.NamedAgg(column=\"timestamp\", aggfunc=lambda x: x.max() - x.min()),\n",
    "    packet_count=pd.NamedAgg(column=\"timestamp\", aggfunc=\"count\")\n",
    ").reset_index()\n",
    "print(\"Flow Statistics:\")\n",
    "print(flow_stats.describe())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "0qFV6q2OCCsK"
   },
   "source": [
    "**Write a brief justification for the features that you have chosen.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "lBCP_2SBC2xj"
   },
   "source": [
    "### Inferring Segment downloads\n",
    "\n",
    "In addition to the features that you could generate using the `netml` library or similar, add to your feature vector a \"segment downloads rate\" feature, which indicates the number of video segments downloaded for a given time window.\n",
    "\n",
    "Note: If you are using the `netml` library, generating features with `SAMP` style options may be useful, as this option gives you time windows, and you can then simply add the segment download rate to that existing dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#add to your feature vector a \"segment downloads rate\" feature, which indicates the number of video segments downloaded for a given time window.\n",
    "nflx_pkts[\"time_window\"] = (nflx_pkts[\"timestamp\"] // 10) * 10  # 10-second windows\n",
    "segment_download_rate = nflx_pkts.groupby(\"time_window\").size().reset_index(name=\"segment_download_rate\")\n",
    "nflx_pkts = nflx_pkts.merge(segment_download_rate, on=\"time_window\", how=\"left\")\n",
    "print(nflx_pkts[[\"timestamp\", \"time_window\", \"segment_download_rate\"]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Video Quality Inference\n",
    "\n",
    "You will now load the complete video dataset from a previous study to train and test models based on these features to automatically infer the quality of a streaming video flow.\n",
    "\n",
    "For this part of the assignment, you will need two pickle files, which we provide for you by running the code below:\n",
    "\n",
    "```\n",
    "\n",
    "!gdown 'https://drive.google.com/uc?id=1N-Cf4dJ3fpak_AWgO05Fopq_XPYLVqdS' -O netflix_session.pkl\n",
    "!gdown 'https://drive.google.com/uc?id=1PHvEID7My6VZXZveCpQYy3lMo9RvMNTI' -O video_dataset.pkl\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the File\n",
    "\n",
    "Load the video dataset pickle file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!gdown 'https://drive.google.com/uc?id=1N-Cf4dJ3fpak_AWgO05Fopq_XPYLVqdS' -O netflix_session.pkl\n",
    "!gdown 'https://drive.google.com/uc?id=1PHvEID7My6VZXZveCpQYy3lMo9RvMNTI' -O video_dataset.pkl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clean the File\n",
    "\n",
    "1. The dataset contains video resolutions that are not valid. Remove entries in the dataset that do not contain a valid video resolution. Valid resolutions are 240, 360, 480, 720, 1080."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import pandas as pd\n",
    "\n",
    "with open('../../myData/netflix_dataset.pkl', 'rb') as f:\n",
    "    session_data = pickle.load(f)\n",
    "\n",
    "print(session_data.columns.tolist())\n",
    "print(session_data.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. The file also contains columns that are unnecessary (in fact, unhelpful!) for performing predictions. Identify those columns, and remove them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Load and clean resolution\n",
    "file_path = \"../../myData/netflix_dataset.pkl\"\n",
    "df = pd.read_pickle(file_path)\n",
    "valid_resolutions = [240, 360, 480, 720, 1080]\n",
    "df_clean = df[df[\"resolution\"].isin(valid_resolutions)].copy()\n",
    "\n",
    "print(f\"Original columns: {df_clean.shape[1]}\")\n",
    "\n",
    "# Identify columns to remove\n",
    "columns_to_remove = []\n",
    "\n",
    "# 2. Remove zero-variance columns (only for non-object types)\n",
    "zero_var_cols = []\n",
    "for col in df_clean.columns:\n",
    "    try:\n",
    "        # Only check numeric columns\n",
    "        if df_clean[col].dtype in ['int64', 'float64', 'bool']:\n",
    "            if df_clean[col].nunique() <= 1:\n",
    "                zero_var_cols.append(col)\n",
    "    except:\n",
    "        pass\n",
    "columns_to_remove.extend(zero_var_cols)\n",
    "print(f\"Zero variance columns: {zero_var_cols}\")\n",
    "\n",
    "# 3. Remove redundant 'R' columns\n",
    "r_cols = [col for col in df_clean.columns if col.endswith('R')]\n",
    "columns_to_remove.extend(r_cols)\n",
    "print(f\"Redundant 'R' columns: {len(r_cols)}\")\n",
    "\n",
    "# 4. Identify nested data structures (for reporting)..not gonna help since we want ints\n",
    "nested_cols = []\n",
    "for col in df_clean.columns:\n",
    "    if df_clean[col].dtype == 'object':\n",
    "        # Check first non-null value\n",
    "        sample = df_clean[col].dropna().iloc[0] if len(df_clean[col].dropna()) > 0 else None\n",
    "        if isinstance(sample, (list, np.ndarray)):\n",
    "            nested_cols.append(col)\n",
    "columns_to_remove.extend(nested_cols)\n",
    "print(f\"Nested columns: {nested_cols}\")\n",
    "\n",
    "# Remove duplicates and drop\n",
    "columns_to_remove = list(set(columns_to_remove))\n",
    "df_final = df_clean.drop(columns=columns_to_remove)\n",
    "\n",
    "print(f\"\\nTotal removed: {len(columns_to_remove)} columns\")\n",
    "print(f\"Final dataset shape: {df_final.shape}\")\n",
    "print(f\"\\nSample of removed columns: {sorted(columns_to_remove)[:10]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Briefly explain why you removed those columns.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare Your Data\n",
    "\n",
    "Prepare your data matrix, determine your features and labels, and perform a train-test split on your data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "# ===== PREPARE FEATURES AND LABELS =====\n",
    "y = df_final['resolution']  # Labels (target) - CHANGED TO RESOLUTION\n",
    "X = df_final.drop(columns=['resolution'])  # Features - DROP RESOLUTION\n",
    "\n",
    "# Remove non-numeric columns\n",
    "X = X.select_dtypes(include=[np.number])\n",
    "\n",
    "print(f\"\\nFeatures (X) shape: {X.shape}\")\n",
    "print(f\"Labels (y) shape: {y.shape}\")\n",
    "print(f\"\\nTarget (resolution) distribution:\")\n",
    "print(y.value_counts().sort_index())\n",
    "\n",
    "# ===== TRAIN-TEST SPLIT =====\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, \n",
    "    test_size=0.2,\n",
    "    random_state=42,\n",
    "    stratify=y  # ADDED: Keep same resolution distribution in train/test\n",
    ")\n",
    "\n",
    "print(f\"\\nTrain-Test Split Complete:\")\n",
    "print(f\"  X_train: {X_train.shape}\")\n",
    "print(f\"  X_test: {X_test.shape}\")\n",
    "print(f\"  y_train: {y_train.shape}\")\n",
    "print(f\"  y_test: {y_test.shape}\")\n",
    "\n",
    "print(f\"\\nResolution distribution in training set:\")\n",
    "print(y_train.value_counts().sort_index())\n",
    "print(f\"\\nResolution distribution in test set:\")\n",
    "print(y_test.value_counts().sort_index())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train and Tune Your Model\n",
    "\n",
    "1. Select a model of your choice.\n",
    "2. Train the model using your training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error\n",
    "\n",
    "# ===== REMOVE NON-NUMERIC COLUMNS FIRST =====\n",
    "print(\"Checking for non-numeric columns...\")\n",
    "print(f\"X_train shape before filtering: {X_train.shape}\")\n",
    "\n",
    "# Keep only numeric columns\n",
    "X_train = X_train.select_dtypes(include=[np.number])\n",
    "X_test = X_test.select_dtypes(include=[np.number])\n",
    "\n",
    "print(f\"X_train shape after filtering: {X_train.shape}\")\n",
    "print(f\"X_test shape after filtering: {X_test.shape}\")\n",
    "\n",
    "# ===== CHECK FOR MISSING VALUES =====\n",
    "print(\"\\nChecking data quality...\")\n",
    "print(f\"Missing values in X_train: {X_train.isnull().sum().sum()}\")\n",
    "print(f\"Missing values in X_test: {X_test.isnull().sum().sum()}\")\n",
    "\n",
    "# Show columns with missing values\n",
    "missing_cols = X_train.columns[X_train.isnull().any()].tolist()\n",
    "if missing_cols:\n",
    "    print(f\"\\nColumns with missing values ({len(missing_cols)}):\")\n",
    "    for col in missing_cols[:10]:  # Show first 10\n",
    "        print(f\"  - {col}: {X_train[col].isnull().sum()} missing\")\n",
    "\n",
    "# ===== HANDLE MISSING VALUES =====\n",
    "imputer = SimpleImputer(strategy='median')\n",
    "\n",
    "print(\"\\nImputing missing values with median...\")\n",
    "X_train_imputed = pd.DataFrame(\n",
    "    imputer.fit_transform(X_train),\n",
    "    columns=X_train.columns,\n",
    "    index=X_train.index\n",
    ")\n",
    "\n",
    "X_test_imputed = pd.DataFrame(\n",
    "    imputer.transform(X_test),\n",
    "    columns=X_test.columns,\n",
    "    index=X_test.index\n",
    ")\n",
    "\n",
    "print(f\"After imputation:\")\n",
    "print(f\"  X_train missing values: {X_train_imputed.isnull().sum().sum()}\")\n",
    "print(f\"  X_test missing values: {X_test_imputed.isnull().sum().sum()}\")\n",
    "\n",
    "# ===== TRAIN LINEAR REGRESSION MODEL =====\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"TRAINING LINEAR REGRESSION MODEL (80/20 SPLIT)\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "model = LinearRegression()\n",
    "\n",
    "print(f\"\\nTraining on {X_train_imputed.shape[0]} samples ({X_train_imputed.shape[0]/(X_train_imputed.shape[0]+X_test_imputed.shape[0])*100:.1f}%)\")\n",
    "print(f\"Testing on {X_test_imputed.shape[0]} samples ({X_test_imputed.shape[0]/(X_train_imputed.shape[0]+X_test_imputed.shape[0])*100:.1f}%)\")\n",
    "print(f\"Features: {X_train_imputed.shape[1]}\")\n",
    "\n",
    "model.fit(X_train_imputed, y_train)\n",
    "\n",
    "print(\"\\n✓ Model training complete!\")\n",
    "\n",
    "# ===== EVALUATE ON TRAINING DATA =====\n",
    "y_train_pred = model.predict(X_train_imputed)\n",
    "\n",
    "train_mse = mean_squared_error(y_train, y_train_pred)\n",
    "train_rmse = np.sqrt(train_mse)\n",
    "train_mae = mean_absolute_error(y_train, y_train_pred)\n",
    "train_r2 = r2_score(y_train, y_train_pred)\n",
    "\n",
    "print(f\"\\n=== Training Set Performance ===\")\n",
    "print(f\"R² Score: {train_r2:.4f}\")\n",
    "print(f\"RMSE: {train_rmse:.4f}p\")\n",
    "print(f\"MAE: {train_mae:.4f}p\")\n",
    "\n",
    "# ===== EVALUATE ON TEST DATA =====\n",
    "y_test_pred = model.predict(X_test_imputed)\n",
    "\n",
    "test_mse = mean_squared_error(y_test, y_test_pred)\n",
    "test_rmse = np.sqrt(test_mse)\n",
    "test_mae = mean_absolute_error(y_test, y_test_pred)\n",
    "test_r2 = r2_score(y_test, y_test_pred)\n",
    "\n",
    "print(f\"\\n=== Test Set Performance ===\")\n",
    "print(f\"R² Score: {test_r2:.4f}\")\n",
    "print(f\"RMSE: {test_rmse:.4f}p\")\n",
    "print(f\"MAE: {test_mae:.4f}p\")\n",
    "\n",
    "# Show prediction examples\n",
    "print(f\"\\n=== Sample Predictions ===\")\n",
    "sample_results = pd.DataFrame({\n",
    "    'Actual Resolution': y_test.head(10).values,\n",
    "    'Predicted Resolution': y_test_pred[:10].round(0)\n",
    "})\n",
    "print(sample_results.to_string(index=False))\n",
    "\n",
    "# ===== FEATURE COEFFICIENTS (Top 10 by absolute value) =====\n",
    "coefficients = pd.DataFrame({\n",
    "    'feature': X_train_imputed.columns,\n",
    "    'coefficient': model.coef_\n",
    "})\n",
    "coefficients['abs_coefficient'] = coefficients['coefficient'].abs()\n",
    "coefficients = coefficients.sort_values('abs_coefficient', ascending=False)\n",
    "\n",
    "print(f\"\\n=== Top 10 Most Influential Features ===\")\n",
    "print(coefficients[['feature', 'coefficient']].head(10).to_string(index=False))\n",
    "print(f\"\\nIntercept: {model.intercept_:.4f}\")\n",
    "\n",
    "# ===== CHECK FOR OVERFITTING =====\n",
    "print(f\"\\n=== Overfitting Check ===\")\n",
    "print(f\"Training R²: {train_r2:.4f}\")\n",
    "print(f\"Test R²: {test_r2:.4f}\")\n",
    "print(f\"Difference: {abs(train_r2 - test_r2):.4f}\")\n",
    "if abs(train_r2 - test_r2) < 0.05:\n",
    "    print(\"✓ Model generalizes well (low overfitting)\")\n",
    "elif abs(train_r2 - test_r2) < 0.1:\n",
    "    print(\"⚠ Slight overfitting detected\")\n",
    "else:\n",
    "    print(\"❌ Significant overfitting detected\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tune Your Model\n",
    "\n",
    "Perform hyperparameter tuning to find optimal parameters for your model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"HYPERPARAMETER TUNING - RIDGE REGRESSION\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Simple Ridge regression with different alpha values\n",
    "ridge = Ridge()\n",
    "\n",
    "param_grid = {\n",
    "    'alpha': [0.1, 1, 10, 100]\n",
    "}\n",
    "\n",
    "# Grid search with 5-fold cross-validation\n",
    "grid_search = GridSearchCV(\n",
    "    ridge,\n",
    "    param_grid,\n",
    "    cv=5,\n",
    "    scoring='r2',\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "print(\"\\nSearching for best alpha parameter...\")\n",
    "grid_search.fit(X_train_imputed, y_train)\n",
    "\n",
    "print(f\"\\nBest alpha: {grid_search.best_params_['alpha']}\")\n",
    "print(f\"Best CV R² score: {grid_search.best_score_:.4f}\")\n",
    "\n",
    "# Evaluate on test set\n",
    "best_model = grid_search.best_estimator_\n",
    "y_pred_tuned = best_model.predict(X_test_imputed)\n",
    "\n",
    "tuned_r2 = r2_score(y_test, y_pred_tuned)\n",
    "tuned_rmse = np.sqrt(mean_squared_error(y_test, y_pred_tuned))\n",
    "tuned_mae = mean_absolute_error(y_test, y_pred_tuned)\n",
    "\n",
    "print(f\"\\n=== Tuned Model Performance ===\")\n",
    "print(f\"R² Score: {tuned_r2:.4f}\")\n",
    "print(f\"RMSE: {tuned_rmse:.4f}p\")\n",
    "print(f\"MAE: {tuned_mae:.4f}p\")\n",
    "\n",
    "print(f\"\\n=== Comparison ===\")\n",
    "print(f\"Baseline R²: {test_r2:.4f}\")\n",
    "print(f\"Tuned R²:    {tuned_r2:.4f}\")\n",
    "print(f\"Improvement: {tuned_r2 - test_r2:.4f}\")\n",
    "\n",
    "print(f\"\\n✓ Best model ready for netflix.pcap predictions!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate Your Model\n",
    "\n",
    "Evaluate your model accuracy according to the following metrics:\n",
    "\n",
    "1. Accuracy\n",
    "2. F1 Score\n",
    "3. Confusion Matrix\n",
    "4. ROC/AUC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, f1_score, confusion_matrix, classification_report\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"MODEL EVALUATION - CLASSIFICATION METRICS\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# ===== CONVERT CONTINUOUS PREDICTIONS TO CLASSES =====\n",
    "print(\"\\nConverting continuous predictions to resolution classes...\")\n",
    "\n",
    "# Define valid resolutions\n",
    "valid_resolutions = [240, 360, 480, 720, 1080]\n",
    "\n",
    "def round_to_nearest_resolution(pred):\n",
    "    \"\"\"Round prediction to nearest valid resolution\"\"\"\n",
    "    return min(valid_resolutions, key=lambda x: abs(x - pred))\n",
    "\n",
    "# Get predictions from best model\n",
    "y_pred_continuous = best_model.predict(X_test_imputed)\n",
    "\n",
    "# Round to nearest valid resolution\n",
    "y_pred_class = np.array([round_to_nearest_resolution(p) for p in y_pred_continuous])\n",
    "y_test_class = y_test.values  # Actual resolutions are already valid\n",
    "\n",
    "print(f\"Sample predictions:\")\n",
    "sample_df = pd.DataFrame({\n",
    "    'Actual': y_test_class[:10],\n",
    "    'Predicted (continuous)': y_pred_continuous[:10].round(1),\n",
    "    'Predicted (rounded)': y_pred_class[:10]\n",
    "})\n",
    "print(sample_df.to_string(index=False))\n",
    "\n",
    "# ===== 1. ACCURACY =====\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"1. ACCURACY\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "accuracy = accuracy_score(y_test_class, y_pred_class)\n",
    "print(f\"\\nAccuracy: {accuracy:.4f} ({accuracy*100:.2f}%)\")\n",
    "print(f\"Correct predictions: {np.sum(y_test_class == y_pred_class)} out of {len(y_test_class)}\")\n",
    "\n",
    "# ===== 2. F1 SCORE =====\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"2. F1 SCORE\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "f1_weighted = f1_score(y_test_class, y_pred_class, average='weighted')\n",
    "f1_macro = f1_score(y_test_class, y_pred_class, average='macro')\n",
    "\n",
    "print(f\"\\nWeighted F1 Score: {f1_weighted:.4f}\")\n",
    "print(f\"Macro F1 Score: {f1_macro:.4f}\")\n",
    "\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test_class, y_pred_class, \n",
    "                           target_names=['240p', '360p', '480p', '720p', '1080p'],\n",
    "                           zero_division=0))\n",
    "\n",
    "# ===== 3. CONFUSION MATRIX =====\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"3. CONFUSION MATRIX\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "cm = confusion_matrix(y_test_class, y_pred_class, labels=valid_resolutions)\n",
    "print(\"\\n\", cm)\n",
    "\n",
    "# Visualize confusion matrix\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
    "           xticklabels=['240p', '360p', '480p', '720p', '1080p'],\n",
    "           yticklabels=['240p', '360p', '480p', '720p', '1080p'],\n",
    "           cbar_kws={'label': 'Count'})\n",
    "plt.title('Confusion Matrix - Resolution Prediction', fontsize=14, fontweight='bold')\n",
    "plt.ylabel('True Resolution', fontsize=12)\n",
    "plt.xlabel('Predicted Resolution', fontsize=12)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Calculate per-class accuracy\n",
    "print(\"\\nPer-Resolution Accuracy:\")\n",
    "for res in valid_resolutions:\n",
    "    mask = y_test_class == res\n",
    "    if np.sum(mask) > 0:\n",
    "        correct = np.sum((y_test_class == res) & (y_pred_class == res))\n",
    "        total = np.sum(mask)\n",
    "        acc = correct / total\n",
    "        print(f\"  {res}p: {acc:.4f} ({correct}/{total})\")\n",
    "\n",
    "# ===== 4. ROC/AUC =====\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"4. ROC/AUC\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# For regression-based predictions, we can't get probability scores directly\n",
    "# But we can calculate a pseudo-probability based on distance to each class\n",
    "\n",
    "from sklearn.preprocessing import label_binarize\n",
    "from sklearn.metrics import roc_curve, auc, roc_auc_score\n",
    "\n",
    "# Convert to binary format for ROC\n",
    "y_test_bin = label_binarize(y_test_class, classes=valid_resolutions)\n",
    "n_classes = len(valid_resolutions)\n",
    "\n",
    "# Calculate pseudo-probabilities based on distance\n",
    "def calculate_pseudo_probabilities(predictions, valid_resolutions):\n",
    "    \"\"\"Convert continuous predictions to pseudo-probabilities\"\"\"\n",
    "    probs = np.zeros((len(predictions), len(valid_resolutions)))\n",
    "    \n",
    "    for i, pred in enumerate(predictions):\n",
    "        # Calculate distances to each resolution\n",
    "        distances = np.abs(np.array(valid_resolutions) - pred)\n",
    "        # Convert to probabilities (closer = higher probability)\n",
    "        # Use softmax-like transformation\n",
    "        probs[i] = np.exp(-distances / 100)  # Scale factor\n",
    "        probs[i] = probs[i] / np.sum(probs[i])  # Normalize\n",
    "    \n",
    "    return probs\n",
    "\n",
    "y_pred_proba = calculate_pseudo_probabilities(y_pred_continuous, valid_resolutions)\n",
    "\n",
    "# Calculate ROC curve and AUC for each class\n",
    "fpr = dict()\n",
    "tpr = dict()\n",
    "roc_auc = dict()\n",
    "\n",
    "for i in range(n_classes):\n",
    "    fpr[i], tpr[i], _ = roc_curve(y_test_bin[:, i], y_pred_proba[:, i])\n",
    "    roc_auc[i] = auc(fpr[i], tpr[i])\n",
    "\n",
    "# Calculate macro-average\n",
    "auc_macro = np.mean(list(roc_auc.values()))\n",
    "\n",
    "print(f\"\\nAUC Scores:\")\n",
    "for i, res in enumerate(valid_resolutions):\n",
    "    print(f\"  {res}p: {roc_auc[i]:.4f}\")\n",
    "print(f\"\\nMacro-average AUC: {auc_macro:.4f}\")\n",
    "\n",
    "# Plot ROC curves\n",
    "plt.figure(figsize=(10, 8))\n",
    "colors = ['blue', 'green', 'orange', 'red', 'purple']\n",
    "\n",
    "for i, (res, color) in enumerate(zip(valid_resolutions, colors)):\n",
    "    plt.plot(fpr[i], tpr[i], color=color, lw=2,\n",
    "            label=f'{res}p (AUC = {roc_auc[i]:.3f})')\n",
    "\n",
    "plt.plot([0, 1], [0, 1], 'k--', lw=2, label='Random Classifier')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate', fontsize=12)\n",
    "plt.ylabel('True Positive Rate', fontsize=12)\n",
    "plt.title('ROC Curves - Resolution Prediction', fontsize=14, fontweight='bold')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.grid(alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# ===== SUMMARY =====\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"EVALUATION SUMMARY\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Accuracy:           {accuracy:.4f} ({accuracy*100:.2f}%)\")\n",
    "print(f\"F1 Score (weighted): {f1_weighted:.4f}\")\n",
    "print(f\"AUC (macro):        {auc_macro:.4f}\")\n",
    "print(f\"R² Score:           {tuned_r2:.4f}\")\n",
    "print(f\"RMSE:               {tuned_rmse:.4f}p\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Predict the Ongoing Resolution of a Real Netflix Session\n",
    "\n",
    "Now that you have your model, it's time to put it in practice!\n",
    "\n",
    "Use a preprocessed Netflix video session to infer **and plot** the resolution at 10-second time intervals."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"PREDICTING RESOLUTION FROM NETFLIX SESSION\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "df_full = pd.read_pickle(\"../../myData/netflix_dataset.pkl\")\n",
    "\n",
    "# Keep only valid resolution values\n",
    "valid_resolutions = [240, 360, 480, 720, 1080]\n",
    "df_sessions = df_full[df_full[\"resolution\"].isin(valid_resolutions)].copy()\n",
    "\n",
    "# Randomly select 50 intervals to simulate a streaming session\n",
    "np.random.seed(42)\n",
    "session_sample = df_sessions.sample(50).reset_index(drop=True)\n",
    "\n",
    "print(f\"Selected 50 intervals from the dataset\")\n",
    "\n",
    "X_session = session_sample.drop(columns=[\"resolution\"])\n",
    "X_session = X_session.select_dtypes(include=[np.number])\n",
    "X_session = X_session.reindex(columns=X_train_imputed.columns, fill_value=0)\n",
    "\n",
    "X_session_imputed = pd.DataFrame(\n",
    "    imputer.transform(X_session),\n",
    "    columns=X_session.columns\n",
    ")\n",
    "\n",
    "predictions = best_model.predict(X_session_imputed)\n",
    "\n",
    "# Round each prediction to the nearest valid resolution\n",
    "predictions = np.array([\n",
    "    min(valid_resolutions, key=lambda x: abs(x - p)) for p in predictions\n",
    "])\n",
    "\n",
    "\n",
    "true_resolutions = session_sample[\"resolution\"].values\n",
    "time_points = np.arange(50) * 10  # every 10 seconds\n",
    "\n",
    "plt.figure(figsize=(14, 7))\n",
    "\n",
    "# Predicted line (blue)\n",
    "plt.plot(\n",
    "    time_points, predictions,\n",
    "    marker=\"o\", linewidth=2, markersize=8, color=\"#1f77b4\",\n",
    "    label=\"Predicted Resolution\"\n",
    ")\n",
    "\n",
    "# Actual line (orange dashed)\n",
    "plt.plot(\n",
    "    time_points, true_resolutions,\n",
    "    marker=\"s\", linestyle=\"--\", linewidth=2, markersize=8, color=\"#ff7f0e\",\n",
    "    label=\"Actual Resolution\"\n",
    ")\n",
    "\n",
    "# Optional: highlight mismatched intervals in red\n",
    "mismatch = predictions != true_resolutions\n",
    "plt.scatter(\n",
    "    time_points[mismatch], true_resolutions[mismatch],\n",
    "    color=\"red\", s=100, zorder=5, label=\"Mismatch Points\"\n",
    ")\n",
    "\n",
    "plt.xlabel(\"Time (seconds)\", fontsize=14, fontweight=\"bold\")\n",
    "plt.ylabel(\"Resolution (p)\", fontsize=14, fontweight=\"bold\")\n",
    "plt.title(\"Netflix Streaming Resolution Over Time (Predicted vs Actual)\",\n",
    "          fontsize=16, fontweight=\"bold\")\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.yticks(valid_resolutions, [f\"{r}p\" for r in valid_resolutions])\n",
    "plt.xlim(left=0)\n",
    "plt.legend(fontsize=12, loc=\"best\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nPrediction summary:\")\n",
    "unique, counts = np.unique(predictions, return_counts=True)\n",
    "for res, count in zip(unique, counts):\n",
    "    print(f\"  {res}p: {count} intervals ({count/50*100:.1f}%)\")\n",
    "\n",
    "accuracy = np.mean(predictions == true_resolutions)\n",
    "print(f\"\\nExact match accuracy: {accuracy*100:.1f}%\")\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyM0Cd1i3qtplCsVAB9qTjxA",
   "collapsed_sections": [],
   "name": "pcap_processing_solution.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
